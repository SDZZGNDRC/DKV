package raft

//
// this is an outline of the API that raft must expose to
// the service (or tester). see comments below for
// each of these functions for more details.
//
// rf = Make(...)
//   create a new Raft server.
// rf.Start(command interface{}) (index, term, isleader)
//   start agreement on a new log entry
// rf.GetState() (term, isLeader)
//   ask a Raft for its current term, and whether it thinks it is leader
// ApplyMsg
//   each time a new entry is committed to the log, each Raft peer
//   should send an ApplyMsg to the service (or tester)
//   in the same server.
//

import (
	//	"bytes"

	"bytes"
	"context"
	"encoding/gob"
	"log"
	"math/rand"
	"net"
	"sync"
	"sync/atomic"
	"time"

	"google.golang.org/grpc"

	pb "github.com/SDZZGNDRC/DKV/proto"
)

// as each Raft peer becomes aware that successive log entries are
// committed, the peer should send an ApplyMsg to the service (or
// tester) on the same server, via the applyCh passed to Make(). set
// CommandValid to true to indicate that the ApplyMsg contains a newly
// committed log entry.
//
// in part 2D you'll want to send other kinds of messages (e.g.,
// snapshots) on the applyCh, but set CommandValid to false for these
// other uses.
type ApplyMsg struct {
	CommandValid bool
	Command      []byte
	CommandIndex int

	// For 2D:
	SnapshotValid bool
	Snapshot      []byte
	SnapshotTerm  int
	SnapshotIndex int
}

const (
	Follower = iota
	Candidate
	Leader
)

const (
	HeartBeatTimeOut = 101
	ElectTimeOutBase = 450
)

// A Go object implementing a single Raft peer.
type Raft struct {
	pb.UnimplementedRaftServer

	mu        sync.Mutex // Lock to protect shared access to this peer's state
	peers     []*RaftEnd // RPC end points of all peers
	persister *Persister // Object to hold this peer's persisted state
	me        int        // this peer's index into peers[]
	dead      int32      // set by Kill()

	// Your data here (2A, 2B, 2C).
	// Look at the paper's Figure 2 for a description of what
	// state a Raft server must maintain.

	currentTerm int
	votedFor    int
	log         []pb.Entry

	nextIndex  []int
	matchIndex []int

	// ä»¥ä¸‹ä¸æ˜¯Figure 2ä¸­çš„field
	voteTimer  *time.Timer
	heartTimer *time.Timer
	rd         *rand.Rand
	role       int

	commitIndex int
	lastApplied int
	applyCh     chan ApplyMsg

	condApply *sync.Cond

	// 2D
	snapShot          []byte // å¿«ç…§
	lastIncludedIndex int    // æ—¥å¿—ä¸­çš„æœ€é«˜ç´¢å¼•
	lastIncludedTerm  int    // æ—¥å¿—ä¸­çš„æœ€é«˜Term
}

func (rf *Raft) Print() {
	DPrintf("raft%v:{currentTerm=%v, role=%v, votedFor=%v}\n", rf.me, rf.currentTerm, rf.role, rf.votedFor)
}

func (rf *Raft) ResetVoteTimer() {
	rdTimeOut := GetRandomElectTimeOut(rf.rd)
	rf.voteTimer.Reset(time.Duration(rdTimeOut) * time.Millisecond)
}

func (rf *Raft) ResetHeartTimer(timeStamp int) {
	rf.heartTimer.Reset(time.Duration(timeStamp) * time.Millisecond)
}

func (rf *Raft) RealLogIdx(vIdx int) int {
	// è°ƒç”¨è¯¥å‡½æ•°éœ€è¦æ˜¯åŠ é”çš„çŠ¶æ€
	return vIdx - rf.lastIncludedIndex
}

func (rf *Raft) VirtualLogIdx(rIdx int) int {
	// è°ƒç”¨è¯¥å‡½æ•°éœ€è¦æ˜¯åŠ é”çš„çŠ¶æ€
	return rIdx + rf.lastIncludedIndex
}

// return currentTerm and whether this server
// believes it is the leader.
func (rf *Raft) GetState() (int, bool) {

	// Your code here (2A).
	rf.mu.Lock()
	// DPrintf("server %v GetState è·å–é”mu", rf.me)
	defer func() {
		rf.mu.Unlock()
		// DPrintf("server %v GetState é‡Šæ”¾é”mu", rf.me)
	}()
	return rf.currentTerm, rf.role == Leader
}

// save Raft's persistent state to stable storage,
// where it can later be retrieved after a crash and restart.
// see paper's Figure 2 for a description of what should be persistent.
// before you've implemented snapshots, you should pass nil as the
// second argument to persister.Save().
// after you've implemented snapshots, pass the current snapshot
// (or nil if there's not yet a snapshot).
func (rf *Raft) persist() {
	// TODO: æŒä¹…åŒ–lastIncludedIndexå’ŒlastIncludedTermæ—¶, æ˜¯å¦éœ€è¦åŠ é”?
	// DPrintf("server %v å¼€å§‹æŒä¹…åŒ–, æœ€åä¸€ä¸ªæŒä¹…åŒ–çš„logä¸º: %v:%v", rf.me, len(rf.log)-1, rf.log[len(rf.log)-1].Cmd)

	w := new(bytes.Buffer)
	e := gob.NewEncoder(w)
	// 2C
	e.Encode(rf.votedFor)
	e.Encode(rf.currentTerm)
	e.Encode(rf.log)
	// 2D
	e.Encode(rf.lastIncludedIndex)
	e.Encode(rf.lastIncludedTerm)
	raftstate := w.Bytes()

	rf.persister.Save(raftstate, rf.snapShot)
}

// restore previously persisted state.
func (rf *Raft) readPersist(data []byte) {
	// ç›®å‰åªåœ¨Makeä¸­è°ƒç”¨, å› æ­¤ä¸éœ€è¦é”
	if len(data) == 0 {
		return
	}
	r := bytes.NewBuffer(data)
	d := gob.NewDecoder(r)

	var votedFor int
	var currentTerm int
	var log []pb.Entry
	var lastIncludedIndex int
	var lastIncludedTerm int
	if d.Decode(&votedFor) != nil ||
		d.Decode(&currentTerm) != nil ||
		d.Decode(&log) != nil ||
		d.Decode(&lastIncludedIndex) != nil ||
		d.Decode(&lastIncludedTerm) != nil {
		DPrintf("server %v readPersist failed\n", rf.me)
	} else {
		// 2C
		rf.votedFor = votedFor
		rf.currentTerm = currentTerm
		rf.log = log
		// 2D
		rf.lastIncludedIndex = lastIncludedIndex
		rf.lastIncludedTerm = lastIncludedTerm

		rf.commitIndex = lastIncludedIndex
		rf.lastApplied = lastIncludedIndex
		DPrintf("server %v  readPersist æˆåŠŸ\n", rf.me)
	}
}

func (rf *Raft) readSnapshot(data []byte) {
	// ç›®å‰åªåœ¨Makeä¸­è°ƒç”¨, å› æ­¤ä¸éœ€è¦é”
	if len(data) == 0 {
		DPrintf("server %v è¯»å–å¿«ç…§å¤±è´¥: æ— å¿«ç…§\n", rf.me)
		return
	}
	rf.snapShot = data
	DPrintf("server %v è¯»å–å¿«ç…§cæˆåŠŸ\n", rf.me)
}

// the service says it has created a snapshot that has
// all info up to and including index. this means the
// service no longer needs the log through (and including)
// that index. Raft should now trim its log as much as possible.
func (rf *Raft) Snapshot(index int, snapshot []byte) {
	// Your code here (2D).
	rf.mu.Lock()
	defer rf.mu.Unlock()

	if rf.commitIndex < index || index <= rf.lastIncludedIndex {
		DPrintf("server %v æ‹’ç»äº† Snapshot è¯·æ±‚, å…¶index=%v, è‡ªèº«commitIndex=%v, lastIncludedIndex=%v\n", rf.me, index, rf.commitIndex, rf.lastIncludedIndex)
		return
	}

	DPrintf("server %v åŒæ„äº† Snapshot è¯·æ±‚, å…¶index=%v, è‡ªèº«commitIndex=%v, åŸæ¥çš„lastIncludedIndex=%v, å¿«ç…§åçš„lastIncludedIndex=%v\n", rf.me, index, rf.commitIndex, rf.lastIncludedIndex, index)

	// ä¿å­˜snapshot
	rf.snapShot = snapshot

	rf.lastIncludedTerm = int(rf.log[rf.RealLogIdx(index)].Term)
	// æˆªæ–­log
	rf.log = rf.log[rf.RealLogIdx(index):] // indexä½ç½®çš„logè¢«å­˜åœ¨0ç´¢å¼•å¤„
	rf.lastIncludedIndex = index
	if rf.lastApplied < index {
		rf.lastApplied = index
	}

	rf.persist()
}

// the tester doesn't halt goroutines created by Raft after each test,
// but it does call the Kill() method. your code can use killed() to
// check whether Kill() has been called. the use of atomic avoids the
// need for a lock.
//
// the issue is that long-running goroutines use memory and may chew
// up CPU time, perhaps causing later tests to fail and generating
// confusing debug output. any goroutine with a long-running loop
// should call killed() to check whether it should stop.
func (rf *Raft) Kill() {
	atomic.StoreInt32(&rf.dead, 1)
	// Your code here, if desired.
}

func (rf *Raft) killed() bool {
	z := atomic.LoadInt32(&rf.dead)
	return z == 1
}

// the service using Raft (e.g. a k/v server) wants to start
// agreement on the next command to be appended to Raft's log. if this
// server isn't the leader, returns false. otherwise start the
// agreement and return immediately. there is no guarantee that this
// command will ever be committed to the Raft log, since the leader
// may fail or lose an election. even if the Raft instance has been killed,
// this function should return gracefully.
//
// the first return value is the index that the command will appear at
// if it's ever committed. the second return value is the current
// term. the third return value is true if this server believes it is
// the leader.
func (rf *Raft) Start(command []byte) (int, int, bool) {
	// Your code here (2B).
	// å¦‚æœä¸æ˜¯leaderè¿”å›false
	rf.mu.Lock()
	// DPrintf("server %v Start è·å–é”mu", rf.me)
	defer func() {
		// DPrintf("server %v Start é‡Šæ”¾é”mu", rf.me)
		rf.ResetHeartTimer(15)
		rf.mu.Unlock()
	}()
	if rf.role != Leader {
		return -1, -1, false
	}

	rf.log = append(rf.log, pb.Entry{Term: int64(rf.currentTerm), Cmd: command})
	DPrintf("leader %v å‡†å¤‡æŒä¹…åŒ–", rf.me)
	rf.persist()

	return rf.VirtualLogIdx(len(rf.log) - 1), rf.currentTerm, true
}

func (rf *Raft) CommitChecker() {
	// æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„commit
	DPrintf("server %v çš„ CommitChecker å¼€å§‹è¿è¡Œ", rf.me)
	for !rf.killed() {
		rf.mu.Lock()
		// DPrintf("server %v CommitChecker è·å–é”mu", rf.me)
		for rf.commitIndex <= rf.lastApplied {
			rf.condApply.Wait()
		}
		msgBuf := make([]*ApplyMsg, 0, rf.commitIndex-rf.lastApplied)
		tmpApplied := rf.lastApplied
		for rf.commitIndex > tmpApplied {
			tmpApplied += 1
			if tmpApplied <= rf.lastIncludedIndex {
				// tmpAppliedå¯èƒ½æ˜¯snapShotä¸­å·²ç»è¢«æˆªæ–­çš„æ—¥å¿—é¡¹, è¿™äº›æ—¥å¿—é¡¹å°±ä¸éœ€è¦å†å‘é€äº†
				continue
			}
			if rf.RealLogIdx(tmpApplied) >= len(rf.log) {
				DPrintf("server %v CommitCheckeræ•°ç»„è¶Šç•Œ: tmpApplied=%v,  rf.RealLogIdx(tmpApplied)=%v>=len(rf.log)=%v, lastIncludedIndex=%v", rf.me, tmpApplied, rf.RealLogIdx(tmpApplied), len(rf.log), rf.lastIncludedIndex)
			}
			msg := &ApplyMsg{
				CommandValid: true,
				Command:      rf.log[rf.RealLogIdx(tmpApplied)].Cmd,
				CommandIndex: tmpApplied,
				SnapshotTerm: int(rf.log[rf.RealLogIdx(tmpApplied)].Term),
			}

			msgBuf = append(msgBuf, msg)
		}
		rf.mu.Unlock()
		// DPrintf("server %v CommitChecker é‡Šæ”¾é”mu", rf.me)

		// æ³¨æ„, åœ¨è§£é”åå¯èƒ½åˆå‡ºç°äº†SnapShotè¿›è€Œä¿®æ”¹äº†rf.lastApplied
		for _, msg := range msgBuf {
			rf.mu.Lock()
			if msg.CommandIndex != rf.lastApplied+1 {
				rf.mu.Unlock()
				continue
			}
			DPrintf("server %v å‡†å¤‡commit, log = %v:%v, lastIncludedIndex=%v", rf.me, msg.CommandIndex, msg.SnapshotTerm, rf.lastIncludedIndex)

			rf.mu.Unlock()
			// æ³¨æ„, åœ¨è§£é”åå¯èƒ½åˆå‡ºç°äº†SnapShotè¿›è€Œä¿®æ”¹äº†rf.lastApplied

			rf.applyCh <- *msg

			rf.mu.Lock()
			if msg.CommandIndex != rf.lastApplied+1 {
				rf.mu.Unlock()
				continue
			}
			rf.lastApplied = msg.CommandIndex
			rf.mu.Unlock()
		}
	}
}

func (rf *Raft) sendInstallSnapshot(serverTo int, args *pb.InstallSnapshotArgs) (reply *pb.InstallSnapshotReply, ok bool) {
	reply, err := rf.peers[serverTo].conn.InstallSnapshot(context.Background(), args)
	return reply, err == nil
}

// InstallSnapshot handler
func (rf *Raft) InstallSnapshot(_ context.Context, args *pb.InstallSnapshotArgs) (reply *pb.InstallSnapshotReply, err error) {
	reply = &pb.InstallSnapshotReply{}
	rf.mu.Lock()
	defer rf.mu.Unlock()

	// DPrintf("server %v InstallSnapshot è·å–é”mu", rf.me)

	// 1. Reply immediately if term < currentTerm
	if args.Term < int64(rf.currentTerm) {
		reply.Term = int64(rf.currentTerm)
		DPrintf("server %v æ‹’ç»æ¥è‡ª %v çš„ InstallSnapshot, æ›´å°çš„Term\n", rf.me, args.LeaderId)

		return reply, nil
	}

	// ä¸éœ€è¦å®ç°åˆ†å—çš„RPC

	if int(args.Term) > rf.currentTerm {
		rf.currentTerm = int(args.Term)
		rf.votedFor = -1
		DPrintf("server %v æ¥å—æ¥è‡ª %v çš„ InstallSnapshot, ä¸”å‘ç°äº†æ›´å¤§çš„Term\n", rf.me, args.LeaderId)
	}

	rf.role = Follower
	rf.ResetVoteTimer()
	DPrintf("server %v æ¥æ”¶åˆ° leader %v çš„InstallSnapshot, é‡è®¾å®šæ—¶å™¨", rf.me, args.LeaderId)

	if args.LastIncludedIndex < int64(rf.lastIncludedIndex) || args.LastIncludedIndex < int64(rf.commitIndex) {
		// 1. å¿«ç…§åè€Œæ¯”å½“å‰çš„ lastIncludedIndex æ›´æ—§, ä¸éœ€è¦å¿«ç…§
		// 2. å¿«ç…§æ¯”å½“å‰çš„ commitIndex æ›´æ—§, ä¸èƒ½å®‰è£…å¿«ç…§
		reply.Term = int64(rf.currentTerm)
		return reply, nil
	}

	// 6. If existing log entry has same index and term as snapshotâ€™s last included entry, retain log entries following it and reply
	hasEntry := false
	rIdx := 0
	for ; rIdx < len(rf.log); rIdx++ {
		if rf.VirtualLogIdx(rIdx) == int(args.LastIncludedIndex) && rf.log[rIdx].Term == args.LastIncludedTerm {
			hasEntry = true
			break
		}
	}

	msg := &ApplyMsg{
		SnapshotValid: true,
		Snapshot:      args.Data,
		SnapshotTerm:  int(args.LastIncludedTerm),
		SnapshotIndex: int(args.LastIncludedIndex),
	}

	if hasEntry {
		DPrintf("server %v InstallSnapshot: args.LastIncludedIndex= %v ä½ç½®å­˜åœ¨, ä¿ç•™åé¢çš„log\n", rf.me, args.LastIncludedIndex)

		rf.log = rf.log[rIdx:]
	} else {
		DPrintf("server %v InstallSnapshot: æ¸…ç©ºlog\n", rf.me)
		rf.log = make([]pb.Entry, 0)
		rf.log = append(rf.log, pb.Entry{Term: int64(rf.lastIncludedTerm), Cmd: args.LastIncludedCmd}) // ç´¢å¼•ä¸º0å¤„å ä½
	}

	// 7. Discard the entire log
	// 8. Reset state machine using snapshot contents (and load snapshotâ€™s cluster configuration)

	rf.snapShot = args.Data
	rf.lastIncludedIndex = int(args.LastIncludedIndex)
	rf.lastIncludedTerm = int(args.LastIncludedTerm)

	if rf.commitIndex < int(args.LastIncludedIndex) {
		rf.commitIndex = int(args.LastIncludedIndex)
	}

	if rf.lastApplied < int(args.LastIncludedIndex) {
		rf.lastApplied = int(args.LastIncludedIndex)
	}

	reply.Term = int64(rf.currentTerm)
	rf.applyCh <- *msg
	rf.persist()
	return reply, nil
}

func (rf *Raft) handleInstallSnapshot(serverTo int) {
	reply := &pb.InstallSnapshotReply{}

	rf.mu.Lock()
	// DPrintf("server %v handleInstallSnapshot è·å–é”mu", rf.me)

	if rf.role != Leader {
		// è‡ªå·±å·²ç»ä¸æ˜¯Laderäº†, è¿”å›
		rf.mu.Unlock()
		return
	}

	args := &pb.InstallSnapshotArgs{
		Term:              int64(rf.currentTerm),
		LeaderId:          int64(rf.me),
		LastIncludedIndex: int64(rf.lastIncludedIndex),
		LastIncludedTerm:  int64(rf.lastIncludedTerm),
		Data:              rf.snapShot,
		LastIncludedCmd:   rf.log[0].Cmd,
	}

	rf.mu.Unlock()
	// DPrintf("server %v handleInstallSnapshot é‡Šæ”¾é”mu", rf.me)

	// å‘é€RPCæ—¶ä¸è¦æŒæœ‰é”
	reply, ok := rf.sendInstallSnapshot(serverTo, args)
	if !ok {
		// RPCå‘é€å¤±è´¥, ä¸‹æ¬¡å†è§¦å‘å³å¯
		return
	}

	rf.mu.Lock()
	// DPrintf("server %v handleInstallSnapshot è·å–é”mu", rf.me)
	defer func() {
		// DPrintf("server %v handleInstallSnapshot é‡Šæ”¾é”mu", rf.me)
		rf.mu.Unlock()
	}()

	if rf.role != Leader || int64(rf.currentTerm) != args.Term {
		// å·²ç»ä¸æ˜¯Leaderæˆ–è€…æ˜¯è¿‡æœŸçš„Leader
		return
	}

	if reply.Term > int64(rf.currentTerm) {
		// è‡ªå·±æ˜¯æ—§Leader
		rf.currentTerm = int(reply.Term)
		rf.role = Follower
		rf.votedFor = -1
		rf.ResetVoteTimer()
		rf.persist()
		return
	}

	// LastIncludedIndexå¯èƒ½åŒ…æ‹¬äº†è¿˜æ²¡æœ‰å¤åˆ¶çš„æ—¥å¿—é¡¹, è¿™äº›æ—¥å¿—é¡¹å¯ä»¥ä¸ç”¨å¤åˆ¶äº†
	if rf.matchIndex[serverTo] < int(args.LastIncludedIndex) {
		rf.matchIndex[serverTo] = int(args.LastIncludedIndex)
	}
	rf.nextIndex[serverTo] = rf.matchIndex[serverTo] + 1
}

// example RequestVote RPC reply structure.
// field names must start with capital letters!

func (rf *Raft) sendAppendEntries(serverTo int, args *pb.AppendEntriesArgs) (reply *pb.AppendEntriesReply, ok bool) {
	reply, err := rf.peers[serverTo].conn.AppendEntries(context.Background(), args)
	return reply, err == nil
}

// AppendEntries handler
func (rf *Raft) AppendEntries(_ context.Context, args *pb.AppendEntriesArgs) (reply *pb.AppendEntriesReply, err error) {
	reply = &pb.AppendEntriesReply{}
	// Your code here (2A, 2B).
	// æ–°leaderå‘é€çš„ç¬¬ä¸€ä¸ªæ¶ˆæ¯

	rf.mu.Lock()
	// DPrintf("server %v AppendEntries è·å–é”mu", rf.me)
	defer func() {
		// DPrintf("server %v AppendEntries é‡Šæ”¾é”mu", rf.me)
		rf.mu.Unlock()
	}()

	if args.Term < int64(rf.currentTerm) {
		// 1. Reply false if term < currentTerm (Â§5.1)
		// æœ‰2ç§æƒ…å†µ:
		// - è¿™æ˜¯çœŸæ­£çš„æ¥è‡ªæ—§çš„leaderçš„æ¶ˆæ¯
		// - å½“å‰èŠ‚ç‚¹æ˜¯ä¸€ä¸ªå­¤ç«‹èŠ‚ç‚¹, å› ä¸ºæŒç»­å¢åŠ  currentTerm è¿›è¡Œé€‰ä¸¾, å› æ­¤çœŸæ­£çš„leaderè¿”å›äº†æ›´æ—§çš„term
		reply.Term = int64(rf.currentTerm)
		reply.Success = false
		DPrintf("server %v æ”¶åˆ°äº†æ—§çš„leader% v çš„å¿ƒè·³å‡½æ•°, args=%+v, æ›´æ–°çš„term: %v\n", rf.me, args.LeaderId, args, reply.Term)
		return reply, nil
	}

	// ä»£ç æ‰§è¡Œåˆ°è¿™é‡Œå°±æ˜¯ args.Term >= rf.currentTerm çš„æƒ…å†µ

	// ä¸æ˜¯æ—§ leaderçš„è¯éœ€è¦è®°å½•è®¿é—®æ—¶é—´
	rf.ResetVoteTimer()

	if int(args.Term) > rf.currentTerm {
		// æ–°leaderçš„ç¬¬ä¸€ä¸ªæ¶ˆæ¯
		rf.currentTerm = int(args.Term) // æ›´æ–°iterm
		rf.votedFor = -1                // æ˜“é”™ç‚¹: æ›´æ–°æŠ•ç¥¨è®°å½•ä¸ºæœªæŠ•ç¥¨
		rf.role = Follower
		rf.persist()
	}

	if len(args.Entries) == 0 {
		// å¿ƒè·³å‡½æ•°
		DPrintf("server %v æ¥æ”¶åˆ° leader %v çš„å¿ƒè·³, è‡ªèº«lastIncludedIndex=%v, PrevLogIndex=%v, len(Entries) = %v\n", rf.me, args.LeaderId, rf.lastIncludedIndex, args.PrevLogIndex, len(args.Entries))
	} else {
		DPrintf("server %v æ”¶åˆ° leader %v çš„AppendEntries, è‡ªèº«lastIncludedIndex=%v, PrevLogIndex=%v, len(Entries)= %+v \n", rf.me, args.LeaderId, rf.lastIncludedIndex, args.PrevLogIndex, len(args.Entries))
	}

	isConflict := false

	// æ ¡éªŒPrevLogIndexå’ŒPrevLogTermä¸åˆæ³•
	// 2. Reply false if log doesnâ€™t contain an entry at prevLogIndex whose term matches prevLogTerm (Â§5.3)
	if args.PrevLogIndex < int64(rf.lastIncludedIndex) {
		// è¿‡æ—¶çš„RPC, å…¶ PrevLogIndex ç”šè‡³åœ¨lastIncludedIndexä¹‹å‰
		reply.Success = true
		reply.Term = int64(rf.currentTerm)
		return reply, nil
	} else if int(args.PrevLogIndex) >= rf.VirtualLogIdx(len(rf.log)) {
		// PrevLogIndexä½ç½®ä¸å­˜åœ¨æ—¥å¿—é¡¹
		reply.XTerm = -1
		reply.XLen = int64(rf.VirtualLogIdx(len(rf.log))) // Logé•¿åº¦, åŒ…æ‹¬äº†å·²ç»snapShotçš„éƒ¨åˆ†
		isConflict = true
		DPrintf("server %v çš„logåœ¨PrevLogIndex: %v ä½ç½®ä¸å­˜åœ¨æ—¥å¿—é¡¹, Logé•¿åº¦ä¸º%v\n", rf.me, args.PrevLogIndex, reply.XLen)
	} else if rf.log[rf.RealLogIdx(int(args.PrevLogIndex))].Term != args.PrevLogTerm {
		// PrevLogIndexä½ç½®çš„æ—¥å¿—é¡¹å­˜åœ¨, ä½†termä¸åŒ¹é…
		reply.XTerm = rf.log[rf.RealLogIdx(int(args.PrevLogIndex))].Term
		i := args.PrevLogIndex
		for i > int64(rf.commitIndex) && rf.log[rf.RealLogIdx(int(i))].Term == reply.XTerm {
			i -= 1
		}
		reply.XIndex = i + 1
		reply.XLen = int64(rf.VirtualLogIdx(len(rf.log))) // Logé•¿åº¦, åŒ…æ‹¬äº†å·²ç»snapShotçš„éƒ¨åˆ†
		isConflict = true
		DPrintf("server %v çš„logåœ¨PrevLogIndex: %v ä½ç½®Termä¸åŒ¹é…, args.Term=%v, å®é™…çš„term=%v\n", rf.me, args.PrevLogIndex, args.PrevLogTerm, reply.XTerm)
	}

	if isConflict {
		reply.Term = int64(rf.currentTerm)
		reply.Success = false
		return reply, nil
	}
	// 3. If an existing entry conflicts with a new one (same index
	// but different terms), delete the existing entry and all that
	// follow it (Â§5.3)
	// if len(args.Entries) != 0 && len(rf.log) > args.PrevLogIndex+1 && rf.log[args.PrevLogIndex+1].Term != args.Entries[0].Term {
	// 	// å‘ç”Ÿäº†å†²çª, ç§»é™¤å†²çªä½ç½®å¼€å§‹åé¢æ‰€æœ‰çš„å†…å®¹
	// 	DPrintf("server %v çš„logä¸argså‘ç”Ÿå†²çª, è¿›è¡Œç§»é™¤\n", rf.me)
	// 	rf.log = rf.log[:args.PrevLogIndex+1]
	// }
	for idx, log := range args.Entries {
		ridx := rf.RealLogIdx(int(args.PrevLogIndex)) + 1 + idx
		if ridx < len(rf.log) && rf.log[ridx].Term != log.Term {
			// æŸä½ç½®å‘ç”Ÿäº†å†²çª, è¦†ç›–è¿™ä¸ªä½ç½®å¼€å§‹çš„æ‰€æœ‰å†…å®¹
			rf.log = rf.log[:ridx]
			for _, entry := range args.Entries[idx:] {
				rf.log = append(rf.log, pb.Entry{Term: entry.Term, Cmd: entry.Cmd})
			}

			break
		} else if ridx == len(rf.log) {
			// æ²¡æœ‰å‘ç”Ÿå†²çªä½†é•¿åº¦æ›´é•¿äº†, ç›´æ¥æ‹¼æ¥
			for _, entry := range args.Entries[idx:] {
				rf.log = append(rf.log, pb.Entry{Term: entry.Term, Cmd: entry.Cmd})
			}
			break
		}
	}
	if len(args.Entries) != 0 {
		DPrintf("server %v æˆåŠŸè¿›è¡Œapeend, lastApplied=%v, len(log)=%v\n", rf.me, rf.lastApplied, len(rf.log))
	}

	// 4. Append any new entries not already in the log
	// è¡¥å……apeendçš„ä¸šåŠ¡
	rf.persist()

	reply.Success = true
	reply.Term = int64(rf.currentTerm)

	if args.LeaderCommit > int64(rf.commitIndex) {
		// 5.If leaderCommit > commitIndex, set commitIndex = min(leaderCommit, index of last new entry)
		if int(args.LeaderCommit) > rf.VirtualLogIdx(len(rf.log)-1) {
			rf.commitIndex = rf.VirtualLogIdx(len(rf.log) - 1)
		} else {
			rf.commitIndex = int(args.LeaderCommit)
		}
		DPrintf("server %v å”¤é†’æ£€æŸ¥commitçš„åç¨‹, commitIndex=%v, len(log)=%v\n", rf.me, rf.commitIndex, len(rf.log))
		rf.condApply.Signal() // å”¤é†’æ£€æŸ¥commitçš„åç¨‹
	}
	return reply, nil
}

func (rf *Raft) handleAppendEntries(serverTo int, args *pb.AppendEntriesArgs) {
	// ç›®å‰çš„è®¾è®¡, é‡è¯•è‡ªåŠ¨å‘ç”Ÿåœ¨ä¸‹ä¸€æ¬¡å¿ƒè·³å‡½æ•°, æ‰€ä»¥è¿™é‡Œä¸éœ€è¦æ­»å¾ªç¯

	reply, ok := rf.sendAppendEntries(serverTo, args)
	if !ok {
		return
	}

	rf.mu.Lock()
	// DPrintf("server %v handleAppendEntries è·å–é”mu", rf.me)
	defer func() {
		// DPrintf("server %v handleAppendEntries é‡Šæ”¾é”mu", rf.me)
		rf.mu.Unlock()
	}()

	if rf.role != Leader || args.Term != int64(rf.currentTerm) {
		// å‡½æ•°è°ƒç”¨é—´éš™å€¼å˜äº†, å·²ç»ä¸æ˜¯å‘èµ·è¿™ä¸ªè°ƒç”¨æ—¶çš„termäº†
		// è¦å…ˆåˆ¤æ–­termæ˜¯å¦æ”¹å˜, å¦åˆ™åç»­çš„æ›´æ”¹matchIndexç­‰æ˜¯ä¸å®‰å…¨çš„
		return
	}

	if reply.Success {
		// serverå›å¤æˆåŠŸ
		newMatchIdx := int(args.PrevLogIndex) + len(args.Entries)
		if newMatchIdx > rf.matchIndex[serverTo] {
			// æœ‰å¯èƒ½åœ¨æ­¤æœŸé—´è®©followerå®‰è£…äº†å¿«ç…§, å¯¼è‡´ rf.matchIndex[serverTo] æœ¬æ¥å°±æ›´å¤§
			rf.matchIndex[serverTo] = newMatchIdx
		}

		rf.nextIndex[serverTo] = rf.matchIndex[serverTo] + 1

		// éœ€è¦åˆ¤æ–­æ˜¯å¦å¯ä»¥commit
		N := rf.VirtualLogIdx(len(rf.log) - 1)

		DPrintf("leader %v ç¡®å®šNä»¥å†³å®šæ–°çš„commitIndex, lastIncludedIndex=%v, commitIndex=%v", rf.me, rf.lastIncludedIndex, rf.commitIndex)

		for N > rf.commitIndex {
			count := 1 // 1è¡¨ç¤ºåŒ…æ‹¬äº†leaderè‡ªå·±
			for i := 0; i < len(rf.peers); i++ {
				if i == rf.me {
					continue
				}
				if rf.matchIndex[i] >= N && rf.log[rf.RealLogIdx(N)].Term == int64(rf.currentTerm) {
					// TODO: Næœ‰æ²¡æœ‰å¯èƒ½è‡ªå‡åˆ°snapShotä¹‹å‰çš„ç´¢å¼•å¯¼è‡´logå‡ºç°è´Ÿæ•°ç´¢å¼•è¶Šç•Œ?
					// è§£ç­”: éœ€è¦ç¡®ä¿è°ƒç”¨SnapShotæ—¶æ£€æŸ¥ç´¢å¼•æ˜¯å¦è¶…è¿‡commitIndex
					count += 1
				}
			}
			if count > len(rf.peers)/2 {
				// +1 è¡¨ç¤ºåŒ…æ‹¬è‡ªèº«
				// å¦‚æœè‡³å°‘ä¸€åŠçš„followerå›å¤äº†æˆåŠŸ, æ›´æ–°commitIndex
				break
			}
			N -= 1
		}

		rf.commitIndex = N
		rf.condApply.Signal() // å”¤é†’æ£€æŸ¥commitçš„åç¨‹

		return
	}

	if reply.Term > int64(rf.currentTerm) {
		// å›å¤äº†æ›´æ–°çš„term, è¡¨ç¤ºè‡ªå·±å·²ç»ä¸æ˜¯leaderäº†
		DPrintf("server %v æ—§çš„leaderæ”¶åˆ°äº†æ¥è‡ª server % v çš„å¿ƒè·³å‡½æ•°ä¸­æ›´æ–°çš„term: %v, è½¬åŒ–ä¸ºFollower\n", rf.me, serverTo, reply.Term)

		rf.currentTerm = int(reply.Term)
		rf.role = Follower
		rf.votedFor = -1
		rf.ResetVoteTimer()
		rf.persist()
		return
	}

	if reply.Term == int64(rf.currentTerm) && rf.role == Leader {
		// termä»ç„¶ç›¸åŒ, ä¸”è‡ªå·±è¿˜æ˜¯leader, è¡¨åå¯¹åº”çš„followeråœ¨prevLogIndexä½ç½®æ²¡æœ‰ä¸prevLogTermåŒ¹é…çš„é¡¹
		// å¿«é€Ÿå›é€€çš„å¤„ç†
		if reply.XTerm == -1 {
			// PrevLogIndexè¿™ä¸ªä½ç½®åœ¨Followerä¸­ä¸å­˜åœ¨
			DPrintf("leader %v æ”¶åˆ° server %v çš„å›é€€è¯·æ±‚, åŸå› æ˜¯logè¿‡çŸ­, å›é€€å‰çš„nextIndex[%v]=%v, å›é€€åçš„nextIndex[%v]=%v\n", rf.me, serverTo, serverTo, rf.nextIndex[serverTo], serverTo, reply.XLen)
			if rf.lastIncludedIndex >= int(reply.XLen) {
				// ç”±äºsnapshotè¢«æˆªæ–­
				// ä¸‹ä¸€æ¬¡å¿ƒè·³æ·»åŠ InstallSnapshotçš„å¤„ç†
				rf.nextIndex[serverTo] = rf.lastIncludedIndex
			} else {
				rf.nextIndex[serverTo] = int(reply.XLen)
			}
			return
		}

		// é˜²æ­¢æ•°ç»„è¶Šç•Œ
		// if rf.nextIndex[serverTo] < 1 || rf.nextIndex[serverTo] >= len(rf.log) {
		// 	rf.nextIndex[serverTo] = 1
		// }
		i := rf.nextIndex[serverTo] - 1
		if i < rf.lastIncludedIndex {
			i = rf.lastIncludedIndex
		}
		for i > rf.lastIncludedIndex && rf.log[rf.RealLogIdx(i)].Term > reply.XTerm {
			i -= 1
		}

		if i == rf.lastIncludedIndex && rf.log[rf.RealLogIdx(i)].Term > reply.XTerm {
			// è¦æ‰¾çš„ä½ç½®å·²ç»ç”±äºsnapshotè¢«æˆªæ–­
			// ä¸‹ä¸€æ¬¡å¿ƒè·³æ·»åŠ InstallSnapshotçš„å¤„ç†
			rf.nextIndex[serverTo] = rf.lastIncludedIndex
		} else if rf.log[rf.RealLogIdx(i)].Term == reply.XTerm {
			// ä¹‹å‰PrevLogIndexå‘ç”Ÿå†²çªä½ç½®æ—¶, Followerçš„Termè‡ªå·±ä¹Ÿæœ‰

			DPrintf("leader %v æ”¶åˆ° server %v çš„å›é€€è¯·æ±‚, å†²çªä½ç½®çš„Termä¸º%v, serverçš„è¿™ä¸ªTermä»ç´¢å¼•%vå¼€å§‹, è€Œleaderå¯¹åº”çš„æœ€åä¸€ä¸ªXTermç´¢å¼•ä¸º%v, å›é€€å‰çš„nextIndex[%v]=%v, å›é€€åçš„nextIndex[%v]=%v\n", rf.me, serverTo, reply.XTerm, reply.XIndex, i, serverTo, rf.nextIndex[serverTo], serverTo, i+1)
			rf.nextIndex[serverTo] = i + 1 // i + 1æ˜¯ç¡®ä¿æ²¡æœ‰è¢«æˆªæ–­çš„
		} else {
			// ä¹‹å‰PrevLogIndexå‘ç”Ÿå†²çªä½ç½®æ—¶, Followerçš„Termè‡ªå·±æ²¡æœ‰
			DPrintf("leader %v æ”¶åˆ° server %v çš„å›é€€è¯·æ±‚, å†²çªä½ç½®çš„Termä¸º%v, serverçš„è¿™ä¸ªTermä»ç´¢å¼•%vå¼€å§‹, è€Œleaderå¯¹åº”çš„XTermä¸å­˜åœ¨, å›é€€å‰çš„nextIndex[%v]=%v, å›é€€åçš„nextIndex[%v]=%v\n", rf.me, serverTo, reply.XTerm, reply.XIndex, serverTo, rf.nextIndex[serverTo], serverTo, reply.XIndex)
			if int(reply.XIndex) <= rf.lastIncludedIndex {
				// XIndexä½ç½®ä¹Ÿè¢«æˆªæ–­äº†
				// æ·»åŠ InstallSnapshotçš„å¤„ç†
				rf.nextIndex[serverTo] = rf.lastIncludedIndex
			} else {
				rf.nextIndex[serverTo] = int(reply.XIndex)
			}
		}
		return
	}
}

func (rf *Raft) SendHeartBeats() {
	// 2Bç›¸å¯¹2Açš„å˜åŒ–, çœŸå®çš„AppendEntriesä¹Ÿé€šè¿‡å¿ƒè·³å‘é€
	DPrintf("leader %v å¼€å§‹å‘é€å¿ƒè·³\n", rf.me)

	for !rf.killed() {
		<-rf.heartTimer.C
		rf.mu.Lock()
		// DPrintf("server %v SendHeartBeats è·å–é”mu", rf.me)
		// if the server is dead or is not the leader, just return
		if rf.role != Leader {
			rf.mu.Unlock()
			// DPrintf("server %v SendHeartBeats é‡Šæ”¾é”mu", rf.me)
			// ä¸æ˜¯leaderåˆ™ç»ˆæ­¢å¿ƒè·³çš„å‘é€
			return
		}

		for i := 0; i < len(rf.peers); i++ {
			if i == rf.me {
				continue
			}
			args := &pb.AppendEntriesArgs{
				Term:         int64(rf.currentTerm),
				LeaderId:     int64(rf.me),
				PrevLogIndex: int64(rf.nextIndex[i] - 1),
				LeaderCommit: int64(rf.commitIndex),
			}

			sendInstallSnapshot := false

			if args.PrevLogIndex < int64(rf.lastIncludedIndex) {
				// è¡¨ç¤ºFolloweræœ‰è½åçš„éƒ¨åˆ†ä¸”è¢«æˆªæ–­, æ”¹ä¸ºå‘é€åŒæ­¥å¿ƒè·³
				DPrintf("leader %v å–æ¶ˆå‘ server %v å¹¿æ’­æ–°çš„å¿ƒè·³, æ”¹ä¸ºå‘é€sendInstallSnapshot, lastIncludedIndex=%v, nextIndex[%v]=%v, args = %+v \n", rf.me, i, rf.lastIncludedIndex, i, rf.nextIndex[i], args)
				sendInstallSnapshot = true
			} else if rf.VirtualLogIdx(len(rf.log)-1) > int(args.PrevLogIndex) {
				// å¦‚æœæœ‰æ–°çš„logéœ€è¦å‘é€, åˆ™å°±æ˜¯ä¸€ä¸ªçœŸæ­£çš„AppendEntriesè€Œä¸æ˜¯å¿ƒè·³

				startIdx := rf.RealLogIdx(int(args.PrevLogIndex) + 1)
				entries := make([]*pb.Entry, 0)
				for i := startIdx; i < len(rf.log); i++ {
					entries = append(entries, &rf.log[i])
				}
				args.Entries = entries
				DPrintf("leader %v å¼€å§‹å‘ server %v å¹¿æ’­æ–°çš„AppendEntries, lastIncludedIndex=%v, nextIndex[%v]=%v, PrevLogIndex=%v, len(Entries) = %v\n", rf.me, i, rf.lastIncludedIndex, i, rf.nextIndex[i], args.PrevLogIndex, len(args.Entries))
			} else {
				// å¦‚æœæ²¡æœ‰æ–°çš„logå‘é€, å°±å‘é€ä¸€ä¸ªé•¿åº¦ä¸º0çš„åˆ‡ç‰‡, è¡¨ç¤ºå¿ƒè·³
				DPrintf("leader %v å¼€å§‹å‘ server %v å¹¿æ’­æ–°çš„å¿ƒè·³, lastIncludedIndex=%v, nextIndex[%v]=%v, PrevLogIndex=%v, len(Entries) = %v \n", rf.me, i, rf.lastIncludedIndex, i, rf.nextIndex[i], args.PrevLogIndex, len(args.Entries))
				args.Entries = make([]*pb.Entry, 0)
			}

			if sendInstallSnapshot {
				go rf.handleInstallSnapshot(i)
			} else {
				args.PrevLogTerm = rf.log[rf.RealLogIdx(int(args.PrevLogIndex))].Term
				go rf.handleAppendEntries(i, args)
			}
		}

		rf.mu.Unlock()
		// DPrintf("server %v SendHeartBeats é‡Šæ”¾é”mu", rf.me)
		rf.ResetHeartTimer(HeartBeatTimeOut)
	}
}

// code to send a RequestVote RPC to a server.
// server is the index of the target server in rf.peers[].
// expects RPC arguments in args.
// fills in *reply with RPC reply, so caller should
// pass &reply.
// the types of the args and reply passed to Call() must be
// the same as the types of the arguments declared in the
// handler function (including whether they are pointers).
//
// The labrpc package simulates a lossy network, in which servers
// may be unreachable, and in which requests and replies may be lost.
// Call() sends a request and waits for a reply. If a reply arrives
// within a timeout interval, Call() returns true; otherwise
// Call() returns false. Thus Call() may not return for a while.
// A false return can be caused by a dead server, a live server that
// can't be reached, a lost request, or a lost reply.
//
// Call() is guaranteed to return (perhaps after a delay) *except* if the
// handler function on the server side does not return.  Thus there
// is no need to implement your own timeouts around Call().
//
// look at the comments in ../labrpc/labrpc.go for more details.
//
// if you're having trouble getting RPC to work, check that you've
// capitalized all field names in structs passed over RPC, and
// that the caller passes the address of the reply struct with &, not
// the struct itself.
func (rf *Raft) sendRequestVote(server int, args *pb.RequestVoteArgs) (reply *pb.RequestVoteReply, ok bool) {
	reply, err := rf.peers[server].conn.RequestVote(context.Background(), args)
	return reply, err == nil
}

// RequestVote RPC handler.
func (rf *Raft) RequestVote(_ context.Context, args *pb.RequestVoteArgs) (reply *pb.RequestVoteReply, err error) {
	// Your code here (2A, 2B).
	reply = &pb.RequestVoteReply{}

	rf.mu.Lock()
	// DPrintf("server %v RequestVote è·å–é”mu", rf.me)
	defer func() {
		// DPrintf("server %v RequestVote é‡Šæ”¾é”mu", rf.me)
		rf.mu.Unlock()
	}()

	if args.Term < int64(rf.currentTerm) {
		// æ—§çš„term
		// 1. Reply false if term < currentTerm (Â§5.1)
		reply.Term = int64(rf.currentTerm)
		reply.VoteGranted = false
		DPrintf("server %v æ‹’ç»å‘ server %v æŠ•ç¥¨: æ—§çš„term: %v, args = %+v\n", rf.me, args.CandidateId, args.Term, args)
		return reply, nil
	}

	// ä»£ç åˆ°è¿™é‡Œæ—¶, args.Term >= rf.currentTerm

	if args.Term > int64(rf.currentTerm) {
		// å·²ç»æ˜¯æ–°ä¸€è½®çš„term, ä¹‹å‰çš„æŠ•ç¥¨è®°å½•ä½œåºŸ
		rf.currentTerm = int(args.Term) // æ›´æ–°åˆ°æ›´æ–°çš„term
		rf.votedFor = -1
		rf.role = Follower
		rf.persist()
	}

	// at least as up-to-date as receiverâ€™s log, grant vote (Â§5.2, Â§5.4)
	if rf.votedFor == -1 || int64(rf.votedFor) == args.CandidateId {
		// é¦–å…ˆç¡®ä¿æ˜¯æ²¡æŠ•è¿‡ç¥¨çš„
		if args.LastLogTerm > rf.log[len(rf.log)-1].Term ||
			(args.LastLogTerm == rf.log[len(rf.log)-1].Term && args.LastLogIndex >= int64(rf.VirtualLogIdx(len(rf.log)-1))) {
			// 2. If votedFor is null or candidateId, and candidateâ€™s log is least as up-to-date as receiverâ€™s log, grant vote (Â§5.2, Â§5.4)
			rf.currentTerm = int(args.Term)
			reply.Term = int64(rf.currentTerm)
			rf.votedFor = int(args.CandidateId)
			rf.role = Follower
			rf.ResetVoteTimer()
			rf.persist()

			reply.VoteGranted = true
			DPrintf("server %v åŒæ„å‘ server %v æŠ•ç¥¨, args = %+v, len(rf.log)=%v\n", rf.me, args.CandidateId, args, len(rf.log))
			return
		} else {
			if args.LastLogTerm < rf.log[len(rf.log)-1].Term {
				DPrintf("server %v æ‹’ç»å‘ server %v æŠ•ç¥¨: æ›´æ—§çš„LastLogTerm, args = %+v\n", rf.me, args.CandidateId, args)
			} else {
				DPrintf("server %v æ‹’ç»å‘ server %v æŠ•ç¥¨: æ›´çŸ­çš„Log, args = %+v\n", rf.me, args.CandidateId, args)
			}
		}
	} else {
		DPrintf("server %v æ‹’ç»å‘ server %væŠ•ç¥¨: å·²æŠ•ç¥¨, args = %+v\n", rf.me, args.CandidateId, args)
	}

	reply.Term = int64(rf.currentTerm)
	reply.VoteGranted = false
	return reply, nil
}

func (rf *Raft) GetVoteAnswer(server int, args *pb.RequestVoteArgs) bool {
	// sendArgs := *args
	// reply := pb.RequestVoteReply{}
	reply, ok := rf.sendRequestVote(server, args)
	if !ok {
		return false
	}

	rf.mu.Lock()
	// DPrintf("server %v GetVoteAnswer è·å–é”mu", rf.me)
	defer func() {
		// DPrintf("server %v GetVoteAnswer é‡Šæ”¾é”mu", rf.me)
		rf.mu.Unlock()
	}()

	if rf.role != Candidate || args.Term != int64(rf.currentTerm) {
		// æ˜“é”™ç‚¹: å‡½æ•°è°ƒç”¨çš„é—´éš™è¢«ä¿®æ”¹äº†
		return false
	}

	if reply.Term > int64(rf.currentTerm) {
		// å·²ç»æ˜¯è¿‡æ—¶çš„termäº†
		rf.currentTerm = int(reply.Term)
		rf.votedFor = -1
		rf.role = Follower
		rf.persist()
	}
	return reply.VoteGranted
}

func (rf *Raft) collectVote(serverTo int, args *pb.RequestVoteArgs, muVote *sync.Mutex, voteCount *int) {
	voteAnswer := rf.GetVoteAnswer(serverTo, args)
	if !voteAnswer {
		return
	}
	muVote.Lock()
	if *voteCount > len(rf.peers)/2 {
		muVote.Unlock()
		return
	}

	*voteCount += 1
	if *voteCount > len(rf.peers)/2 {
		rf.mu.Lock()
		// DPrintf("server %v collectVote è·å–é”mu", rf.me)
		if rf.role != Candidate || rf.currentTerm != int(args.Term) {
			// æœ‰å¦å¤–ä¸€ä¸ªæŠ•ç¥¨çš„åç¨‹æ”¶åˆ°äº†æ›´æ–°çš„termè€Œæ›´æ”¹äº†è‡ªèº«çŠ¶æ€ä¸ºFollower
			// æˆ–è€…è‡ªå·±çš„termå·²ç»è¿‡æœŸäº†, ä¹Ÿå°±æ˜¯è¢«æ–°ä¸€è½®çš„é€‰ä¸¾è¿½ä¸Šäº†
			rf.mu.Unlock()
			// DPrintf("server %v é‡Šæ”¾é”mu", rf.me)

			muVote.Unlock()
			return
		}
		DPrintf("server %v æˆä¸ºäº†æ–°çš„ leader", rf.me)
		rf.role = Leader
		// éœ€è¦é‡æ–°åˆå§‹åŒ–nextIndexå’ŒmatchIndex
		for i := 0; i < len(rf.nextIndex); i++ {
			rf.nextIndex[i] = rf.VirtualLogIdx(len(rf.log))
			rf.matchIndex[i] = rf.lastIncludedIndex // ç”±äºmatchIndexåˆå§‹åŒ–ä¸ºlastIncludedIndex, å› æ­¤åœ¨å´©æºƒæ¢å¤å, å¤§æ¦‚ç‡è§¦å‘InstallSnapshot RPC
		}
		rf.mu.Unlock()
		// DPrintf("server %v collectVote é‡Šæ”¾é”mu", rf.me)

		go rf.SendHeartBeats()
	}

	muVote.Unlock()
}

func (rf *Raft) Elect() {
	// ç‰¹åˆ«æ³¨æ„, è¦å…ˆå¯¹muVoteåŠ é”, å†å¯¹muåŠ é”, è¿™æ˜¯ä¸ºäº†ç»Ÿä¸€è·å–é”çš„é¡ºåºä»¥é¿å…æ­»é”

	rf.mu.Lock()
	// DPrintf("server %v Elect è·å–é”mu", rf.me)
	defer func() {
		// DPrintf("server %v Elect é‡Šæ”¾é”mu", rf.me)
		rf.mu.Unlock()
	}()

	rf.currentTerm += 1 // è‡ªå¢term
	rf.role = Candidate // æˆä¸ºå€™é€‰äºº
	rf.votedFor = rf.me // ç»™è‡ªå·±æŠ•ç¥¨
	rf.persist()

	voteCount := 1 // è‡ªå·±æœ‰ä¸€ç¥¨
	var muVote sync.Mutex

	DPrintf("server %v å¼€å§‹å‘èµ·æ–°ä¸€è½®æŠ•ç¥¨, æ–°ä¸€è½®çš„termä¸º: %v", rf.me, rf.currentTerm)

	args := &pb.RequestVoteArgs{
		Term:         int64(rf.currentTerm),
		CandidateId:  int64(rf.me),
		LastLogIndex: int64(rf.VirtualLogIdx(len(rf.log) - 1)),
		LastLogTerm:  rf.log[len(rf.log)-1].Term,
	}

	for i := 0; i < len(rf.peers); i++ {
		if i == rf.me {
			continue
		}
		go rf.collectVote(i, args, &muVote, &voteCount)
	}
}

func (rf *Raft) ticker() {
	for !rf.killed() {
		// Your code here (2A)
		// Check if a leader election should be started.

		// pause for a random amount of time between 50 and 350
		// milliseconds.
		<-rf.voteTimer.C
		rf.mu.Lock()
		// DPrintf("server %v ticker è·å–é”mu", rf.me)
		if rf.role != Leader {
			// è¶…æ—¶
			go rf.Elect()
		}
		rf.ResetVoteTimer()
		rf.mu.Unlock()
		// DPrintf("server %v ticker é‡Šæ”¾é”mu", rf.me)
	}
}

// the service or tester wants to create a Raft server. the ports
// of all the Raft servers (including this one) are in peers[]. this
// server's port is peers[me]. all the servers' peers[] arrays
// have the same order. persister is a place for this server to
// save its persistent state, and also initially holds the most
// recent saved state, if any. applyCh is a channel on which the
// tester or service expects Raft to send ApplyMsg messages.
// Make() must return quickly, so it should start goroutines
// for any long-running work.
func Make(me int,
	persister *Persister, applyCh chan ApplyMsg, raftAddrs RaftAddrs) *Raft {
	DPrintf("server %v è°ƒç”¨Makeå¯åŠ¨", me)

	rf := &Raft{}
	rf.persister = persister
	rf.me = me

	rf.log = make([]pb.Entry, 0)
	rf.log = append(rf.log, pb.Entry{Term: 0})

	rf.nextIndex = make([]int, len(raftAddrs.Endpoints))
	rf.matchIndex = make([]int, len(raftAddrs.Endpoints))
	// rf.timeStamp = time.Now()
	rf.role = Follower
	rf.applyCh = applyCh
	rf.condApply = sync.NewCond(&rf.mu)
	rf.rd = rand.New(rand.NewSource(int64(rf.me)))
	rf.voteTimer = time.NewTimer(0)
	rf.heartTimer = time.NewTimer(0)
	rf.ResetVoteTimer()

	// initialize from state persisted before a crash
	// å¦‚æœè¯»å–æˆåŠŸ, å°†è¦†ç›–log, votedForå’ŒcurrentTerm
	rf.readSnapshot(persister.ReadSnapshot())
	rf.readPersist(persister.ReadRaftState())

	for i := 0; i < len(rf.nextIndex); i++ {
		rf.nextIndex[i] = rf.VirtualLogIdx(len(rf.log)) // raftä¸­çš„indexæ˜¯ä»1å¼€å§‹çš„
	}

	// start ticker goroutine to start elections
	go rf.ticker()
	go rf.CommitChecker()

	rf.StartRaft(raftAddrs.Endpoints[me])
	servers := WaitConnect(raftAddrs)
	rf.peers = servers

	return rf
}

func (rt *Raft) StartRaft(conf RaftAddr) {
	// server grpc
	lis, err := net.Listen("tcp", conf.Addr+conf.Port)
	if err != nil {
		log.Fatalln("error: etcd start failed", err)
	}
	gServer := grpc.NewServer()
	pb.RegisterRaftServer(gServer, rt)
	go func() {
		if err := gServer.Serve(lis); err != nil {
			log.Fatalln("failed to serve : ", err.Error())
		}
	}()
}

func WaitConnect(conf RaftAddrs) []*RaftEnd {
	DPrintf("start waiting...")
	var wait sync.WaitGroup
	servers := make([]*RaftEnd, len(conf.Endpoints))
	wait.Add(len(servers) - 1)
	for i := range conf.Endpoints {
		if i == conf.Me {
			continue
		}

		go func(other int, conf RaftAddr) {
			defer wait.Done()
			for {
				r := NewRaftClient(conf)
				if r != nil {
					servers[other] = r
					break
				}
				time.Sleep(time.Millisecond * 500)
			}
		}(i, conf.Endpoints[i])
	}
	wait.Wait()
	DPrintf("ğŸ¦– All %d Connect", len(conf.Endpoints))
	return servers
}
